#!/usr/bin/env python

import sys
import logging
import os
import getopt
import string
import stat
import re

from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from esgcet.publish import extractFromDataset, aggregateVariables, filelistIterator, fnmatchIterator, fnIterator, directoryIterator, multiDirectoryIterator, progressCallback, StopEvent, readDatasetMap, datasetMapIterator, iterateOverDatasets, publishDatasetList, processIterator, processNodeMatchIterator
from esgcet.config import loadConfig, getHandler, getHandlerByName, initLogging, registerHandlers, CFHandler, splitLine, getOfflineLister
from esgcet.exceptions import *
from esgcet.messaging import debug, info, warning, error, critical, exception
from esgcet.model import Dataset
from esgcet.query import queryDatasetMap

usage = """Usage:
    esgpublish [options] directory [directory ...]

    -or-

    esgpublish [options] --map dataset_map

    Extract metadata from a list of directories representing one or more datasets, into a database. The directories should relate to one project only. Optionally generate a THREDDS configuration catalog for each dataset.

Arguments:
    dataset_map: A file mapping dataset_ids to directories, as generated by esgscan_directory.

    directory: Directory path to scan recursively.

Options:

    -a aggregate_dimension_name:
        Name of the aggregate dimension. Defaults to 'time'

    --append
        If a dataset exists, append new files to the dataset.

    --echo-sql: Echo SQL commands

    --dataset dataset_name:
        String name of the dataset. If specified, all files will belong to the specified dataset,
        regardless of path. If omitted, paths are matched to the directory_format, as specified in the
        configuration file, to determine the dataset name.

    --experiment experiment_id:
        Experiment identifier. All datasets will have this experiment ID, regardless of informtion
        in the dataset map or directory names.

    --filter regular_expression:
        Filter files matching the regular expression. The default is '.*\.nc$'
        Regular expression syntax is defined by the Python re module.

    -h, --help: Print a help message.

    -i init_file: Initialization file. If not specified, the default installed init file is read.

    --keep-version:
        Keep the dataset version number the same for an existing dataset. By default the version number
        is incremented by 1. This option is ignored for new datasets. See --version.

    --las
        Reinitialize the LAS (Live Access Server). The default is not to reinitialize.

    --map dataset_map: Read input from a dataset map, as generated by esgscan_directory.
        The directory arguments are ignored.

    --model model_id:
        Model identifier. All datasets will have this model ID, regardless of informtion
        in the dataset map or directory names.

    --noscan
        Skip the scan phase and just publish. Assumes that the scan has already been done!

    --offline
        The datasets are offline. A minimal amount of information is published, including file size.
        The datafiles are not scanned, and no aggregations are published.

        Note: The project_id and dataset_id must be specified with this option (see --project and
        --dataset).

    -p, --property 'name=value':
        Add a property/value pair. This option can be used multiple times.

        Note: the property must also be configured in the initialization file
        and project handler.

    --parent parent_id:
        Name of the parent dataset of ALL the datasets. If not specified, the parent identifier is generated
        for each dataset from the parent_id option of the initialization file. Use this option with caution.

    --per-time
    --per-variable
        Specify how THREDDS catalogs are generated. If per variable, create a dataset and aggregation for
        each variable. If per time, all variables are contained in a single dataset. The options are
        mutually exclusive, and override the configuration option 'variable_per_file'. Offline datasets
        are always written as per time.

    --project project_id:
        Project identifier. If not specified, the project is determined from the dataset map
        if the --map form is used, otherwise the project is determined from the first file found
        that matches the file filter (see --filter).

        Note: This option is mandatory for offline datasets.

    --publish
        Publish the dataset if there are no errors. Implies --thredds.

    --service service_name
        Specify a THREDDS service name to associate with an offline dataset. If omitted, the name of the
        first offline service in the configuration ''thredds_offline_services'' is used. This determines
        which offline lister to use.

    --summarize-errors
        Print a summary of errors for each dataset scanned.

    --thredds
        Generate THREDDS files.

    --use-existing dataset_name
        Run the scan phase based on dataset and file information already in the database.
        This option may be used more than once. Compare with --map, which takes a mapfile.

    --use-list filelist
        Like --use-existing, but read the list of dataset names from a
        file, containing one dataset name per line. If the filelist is '-',
        read from standard input.

    --version version_number
        Specify the dataset version number, a positive integer. If unspecified, the version number is
        set to 1 for new datasets, and is incremented by 1 for existing datasets. Use this option
        with caution, as the version number will apply to all datasets processed. See --keep-version.

"""

def main(argv):

    try:
        args, lastargs = getopt.getopt(argv, "a:hi:p:", ['append', 'dataset=', 'echo-sql', 'experiment=', 'filter=', 'help', 'keep-version', 'las', 'map=', 'model=', 'offline',  'parent=', 'per-time', 'per-variable', 'project=', 'property=', 'publish', 'noscan', 'service=', 'summarize-errors', 'thredds', 'use-existing=', 'use-list=', 'version='])
    except getopt.error:
        print sys.exc_value
        print usage
        sys.exit(0)

    aggregateDimension = "time"
    appendOpt = False
    datasetMapfile = None
    datasetName = None
    echoSql = False
    filefilt = '.*\.nc$'
    init_file = None
    initcontext = {}
    keepVersion = False
    las = False
    offline = False
    parent = None
    perVariable = None
    projectName = None
    properties = {}
    publish = False
    publishOnly = False
    rescan = False
    rescanDatasetName = []
    service = None
    summarizeErrors = False
    testProgress1 = testProgress2 = None
    thredds = False
    version = None
    for flag, arg in args:
        if flag=='-a':
            aggregateDimension = arg
        elif flag=='--append':
            appendOpt = True
        elif flag=='--dataset':
            datasetName = arg
        elif flag=='--echo-sql':
            echoSql = True
        elif flag=='--experiment':
            initcontext['experiment'] = arg
        elif flag=='--filter':
            filefilt = arg
        elif flag in ['-h', '--help']:
            print usage
            sys.exit(0)
        elif flag=='-i':
            init_file = arg
        elif flag=='--keep-version':
            keepVersion = True
        elif flag=='--las':
            las = True
        elif flag=='--map':
            datasetMapfile = arg
        elif flag=='--model':
            initcontext['model'] = arg
        elif flag=='--noscan':
            publishOnly = True
        elif flag=='--offline':
            offline = True
        elif flag=='--parent':
            parent = arg
        elif flag=='--per-time':
            perVariable = False
        elif flag=='--per-variable':
            perVariable = True
        elif flag=='--project':
            projectName = arg
        elif flag in ['-p', '--property']:
            name, value = arg.split('=')
            properties[name] = value
        elif flag=='--publish':
            publish = True
        elif flag=='--service':
            service = arg
        elif flag=='--summarize-errors':
            summarizeErrors = True
        elif flag=='--thredds':
            thredds = True
        elif flag=='--use-existing':
            rescan = True
            rescanDatasetName.append(arg)
        elif flag=='--use-list':
            rescan = True
            if arg=='-':
                namelist=sys.stdin
            else:
                namelist = open(arg)
            for line in namelist.readlines():
                rescanDatasetName.append(line.strip())
        elif flag=='--version':
            try:
                version = string.atoi(arg)
                if version <=0:
                    raise ValueError
            except ValueError:
                raise ESGPublishError("Version number must be a positive integer: %s"%arg)

    # If offline, the project must be specified
    if offline and (datasetMapfile is None) and (projectName is None):
        raise ESGPublishError("Must specify project with --project for offline datasets")

    # Load the configuration and set up a database connection
    config = loadConfig(init_file)
    engine = create_engine(config.get('extract', 'dburl'), echo=echoSql, pool_recycle=3600)
    initLogging('extract', override_sa=engine)
    Session = sessionmaker(bind=engine, autoflush=True, autocommit=False)

    # Register project handlers
    registerHandlers()

    # If the dataset map is input, just read it ...
    dmap = None
    directoryMap = None
    if datasetMapfile is not None:
        dmap = readDatasetMap(datasetMapfile)
        datasetNames = dmap.keys()

    elif rescan:
        dmap, offline = queryDatasetMap(rescanDatasetName, Session)
        datasetNames = rescanDatasetName

    # ... otherwise generate the directory map.
    else:
        # Online dataset(s)
        if not offline:
            if len(lastargs)==0:
                print "No directories specified."
                print usage
                sys.exit(0)

            if projectName is not None:
                handler = getHandlerByName(projectName, None, Session)
            else:
                multiIter = multiDirectoryIterator(lastargs, filefilt=filefilt)
                firstFile, size = multiIter.next()
                listIter = list(multiIter)
                handler = getHandler(firstFile, Session, validate=True)
                if handler is None:
                    raise ESGPublishError("No project found in file %s, specify with --project."%firstFile)
                projectName = handler.name

            props = properties.copy()
            props.update(initcontext)
            directoryMap = handler.generateDirectoryMap(lastargs, filefilt, initContext=props, datasetName=datasetName)
            datasetNames = directoryMap.keys()

        # Offline dataset. Format the spec as a dataset map : dataset_name => [(path, size), (path, size), ...]
        else:
            handler = getHandlerByName(projectName, None, Session, offline=True)
            dmap = {}
            listerSection = getOfflineLister(config, "project:%s"%projectName, service)
            offlineLister = config.get(listerSection, 'offline_lister_executable')
            commandArgs = "--config-section %s "%listerSection
            commandArgs += " ".join(lastargs)
            for dsetName, filepath, size in processNodeMatchIterator(offlineLister, commandArgs, handler, filefilt=filefilt, datasetName=datasetName):
                if dmap.has_key(dsetName):
                    dmap[dsetName].append((filepath, str(size)))
                else:
                    dmap[dsetName] = [(filepath, str(size))]

            datasetNames = dmap.keys()

    datasetNames.sort()
    if len(datasetNames)==0:
        warning("No datasets found.")

    # Iterate over datasets
    if not publishOnly:
        datasets = iterateOverDatasets(projectName, dmap, directoryMap, datasetNames, Session, aggregateDimension, appendOpt, filefilt, initcontext, offline, properties, keepVersion=keepVersion, newVersion=version)

    result = publishDatasetList(datasetNames, Session, publish=publish, thredds=thredds, las=las, parentId=parent, service=service, perVariable=perVariable)
    print `result`

    if summarizeErrors:
        print 'Summary of errors:'
        for name in datasetNames:
            dset = Dataset.lookup(name, Session)
            print dset.get_name(Session), dset.get_project(Session), dset.get_model(Session), dset.get_experiment(Session), dset.get_run_name(Session)
            if dset.has_warnings(Session):
                print '=== Dataset: %s ==='%dset.name
                for line in dset.get_warnings(Session):
                    print line

if __name__=='__main__':
    main(sys.argv[1:])
